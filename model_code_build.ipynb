{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### to explain all process and work flow \n",
    "\n",
    "## its only pose will using to add if any hands or face will that will include  some codes\n",
    "\n",
    "            # if results.multi_hand_landmarks:\n",
    "            # for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "            #     mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS, \n",
    "            #                             mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "            #                             mp_drawing.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2),\n",
    "            #                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to install packages for model \"req_pack\"\n",
    "\n",
    "# ------ Use => pip install -r req_pack.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745340913.771174    3489 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1745340913.777410    4808 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) Graphics (RPL-U)\n",
      "W0000 00:00:1745340913.927915    4794 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745340913.993252    4802 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745340914.829409    4802 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Jump Height: 0.38 meters\n"
     ]
    }
   ],
   "source": [
    "# Normal code \n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# ----------------- Initialize MediaPipe pose module --------------google \n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False,min_detection_confidence=0.5,min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "landmark_index =mp_pose.PoseLandmark.RIGHT_HIP.value\n",
    "y_positions = []\n",
    "\n",
    "# -----------------------------Connect into video cam -----------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    re, frame = cap.read()\n",
    "    if not re:\n",
    "        break\n",
    "    h, w, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = pose.process(frame_rgb)\n",
    "\n",
    "# --------------------- To add pose in human -----------------------\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        landmarks = result.pose_landmarks.landmark\n",
    "        landmark = landmarks[landmark_index]\n",
    "        y_pixel = int(landmark.y * h)\n",
    "        x_pixel = int(landmark.x * w)\n",
    "        y_positions.append(y_pixel)\n",
    "\n",
    "        line_length = 20\n",
    "        start_point = (x_pixel - line_length // 2, y_pixel)\n",
    "        end_point = (x_pixel + line_length // 2, y_pixel)\n",
    "        cv2.line(frame, start_point, end_point, (255, 0, 0), 3)\n",
    "\n",
    "        mp_drawing.draw_landmarks(frame, result.pose_landmarks,mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "# ------------ frame will show jumping distance ------------------------\n",
    "\n",
    "        # cv2.putText(frame, f\"Y Pos: {self.jump_height_meters}\", (30, 60),\n",
    "        #             cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "# --------------- closing windows-------------------------------------\n",
    "    cv2.imshow(\"Jump Estimation\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('c'): \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# ------------------ Distance Finder --------------------------------\n",
    "\n",
    "def estimate_jump_height(y_data, pixel_to_meter_ratio=0.0025):\n",
    "    y_min = min(y_data)  # ---- > top of the jump\n",
    "    y_max = max(y_data)  # -----> crouch or base\n",
    "\n",
    "    pixel_displacement = y_max - y_min\n",
    "    jump_height_meters = pixel_displacement * pixel_to_meter_ratio\n",
    "    return jump_height_meters\n",
    "\n",
    "# ---------------------- Estimate and display ----------------------------\n",
    "jump_height = estimate_jump_height(y_positions)\n",
    "print(f\"Estimated Jump Height: {jump_height:.2f} meters\")  # Result will mention below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Process Flow of the Jump Height Code**\n",
    "\n",
    "---\n",
    "\n",
    "###  **Step 1: Import Libraries**\n",
    "- Load necessary Python libraries:\n",
    "  - cv2 for webcam and drawing.\n",
    "  - mediapipe for pose estimation.\n",
    "  - numpy (though not used here, usually for calculations).\n",
    "\n",
    "---\n",
    "\n",
    "###  **Step 2: Initialize the Pose Detector**\n",
    "- Create a class distance_check.\n",
    "  - Prepare a list to store Y-axis positions of the right hip.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Step 3: Start Webcam Capture**\n",
    "- Use cv2.VideoCapture(0) to start video stream.\n",
    "- Begin a loop to read video frames one by one.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Step 4: Process Each Frame**\n",
    "- Convert each frame from **BGR to RGB**.\n",
    "- Pass the frame to MediaPipe Pose for **pose detection**.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Step 5: Extract and Store Landmark Position**\n",
    "- If landmarks are found:\n",
    "  - Get the position of the **right hip**.\n",
    "  - Convert normalized position to **pixel coordinates**.\n",
    "  - Store the Y-coordinate of the hip in a list for later analysis.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Step 6: Draw Visual Feedback**\n",
    "- Draw a horizontal line at the hip's Y-position.\n",
    "- Draw full body pose landmarks and connections.\n",
    "- Display the annotated frame using cv2.imshow().\n",
    "\n",
    "---\n",
    "\n",
    "###  **Step 7: Break the Loop**\n",
    "- The loop continues in real-time until the user presses **'c'**.\n",
    "- After pressing **'c'**:\n",
    "  - Webcam is closed.\n",
    "  - Display windows are destroyed.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Step 8: Calculate Jump Height**\n",
    "- Use the stored Y-positions of the hip.\n",
    "- Identify the **max and min Y-positions** (i.e., bottom and top of jump).\n",
    "- Calculate **pixel displacement** = max - min.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Step 9: Convert Pixels to Meters**\n",
    "- Use a scaling factor (0.0025) to convert pixels to meters.\n",
    "- Compute:\n",
    "  \n",
    "  jump_height = pixel_displacement * 0.0025\n",
    "  \n",
    "\n",
    "---\n",
    "\n",
    "###  **Step 10: Print the Result**\n",
    "- Display the **estimated jump height in meters** in the console.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745340633.193217    3489 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1745340633.199801    3977 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: Mesa Intel(R) Graphics (RPL-U)\n",
      "W0000 00:00:1745340633.315013    3969 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745340633.364713    3965 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/you/.local/lib/python3.10/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Jump Height: 0.00 meters\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "class distance_check:\n",
    "\n",
    "    def __init__(self, mp_pose):\n",
    "        self.pose = mp_pose.Pose(static_image_mode=False,min_detection_confidence=0.5,min_tracking_confidence=0.5)\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_pose = mp_pose\n",
    "        self.landmark_index = mp_pose.PoseLandmark.RIGHT_HIP.value\n",
    "        self.y_positions = []\n",
    "\n",
    "    def video_capture(self):\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        while True:\n",
    "            re, frame = cap.read()\n",
    "            if not re:\n",
    "                break\n",
    "            h, w, _ = frame.shape\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            result = self.pose.process(frame_rgb)\n",
    "\n",
    "            if result.pose_landmarks:\n",
    "                landmarks = result.pose_landmarks.landmark\n",
    "                landmark = landmarks[self.landmark_index]\n",
    "                y_pixel = int(landmark.y * h)\n",
    "                x_pixel = int(landmark.x * w)\n",
    "                self.y_positions.append(y_pixel)\n",
    "\n",
    "                line_length = 20\n",
    "                start_point = (x_pixel - line_length // 2, y_pixel)\n",
    "                end_point = (x_pixel + line_length // 2, y_pixel)\n",
    "                cv2.line(frame, start_point, end_point, (255, 0, 0), 3)\n",
    "\n",
    "                self.mp_drawing.draw_landmarks(frame, result.pose_landmarks, self.mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "                # cv2.putText(frame, f\"Y Pos: {self.jump_height_meters}\", (30, 60),\n",
    "                #             cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "            cv2.imshow(\"Jump Estimation\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('c'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def estimate_jump_height(self, y_data, pixel_to_meter_ratio=0.0025):\n",
    "        if not y_data:\n",
    "            return 0.0\n",
    "        y_min = min(y_data)  \n",
    "        y_max = max(y_data) \n",
    "        pixel_displacement = y_max - y_min\n",
    "        jump_height_meters = pixel_displacement * pixel_to_meter_ratio\n",
    "        return jump_height_meters\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "estimator = distance_check(mp_pose)  \n",
    "estimator.video_capture()\n",
    "\n",
    "# Estimate and print jump height\n",
    "jump_height = estimator.estimate_jump_height(estimator.y_positions)\n",
    "\n",
    "print(f\"Estimated Jump Height: {jump_height:.2f} meters\") # Result will mention below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

